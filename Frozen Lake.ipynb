{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning With the Frozen Lake Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "#### Example Board:\n",
    "* SFFF       (S: starting point, safe)\n",
    "* FHFH       (F: frozen surface, safe)\n",
    "* FFFH       (H: hole, fall to your doom)\n",
    "* HFFG       (G: goal, where the frisbee is located)\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "###### https://gym.openai.com/envs/FrozenLake-v0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Game Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/anaconda3/envs/deepLearning/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "environment = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = environment.action_space.n\n",
    "state_space = environment.observation_space.n\n",
    "q_table = np.zeros((state_space, action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning HyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 20000\n",
    "learning_rate = 0.2\n",
    "max_steps = 100\n",
    "discount_rate = 0.99\n",
    "\n",
    "# Exploitation vs Exploration params.\n",
    "exploration_rate = 1\n",
    "max_exploration_probability = 1\n",
    "min_exploration_probability = 0.01\n",
    "exploration_probability_decay_rate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Algorithm\n",
    "* Continue learning until either the algorithm runs out of time(steps) or the game ends (death, reaches end, etc.).\n",
    "* For each step choose some action (a) from the state (s) based on the values stored in the q-table (Q(s,a)).\n",
    "* Perform chosen action and observe new state (s') and reward (r).\n",
    "* Update q-table using the Bellman equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb25ae01e9f4b48b11d2d4bdcf379da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 episodes of training the average reward was: 0.326 .\n",
      "After 1000 episodes of training the average reward was: 0.651 .\n",
      "After 2000 episodes of training the average reward was: 0.671 .\n",
      "After 3000 episodes of training the average reward was: 0.668 .\n",
      "After 4000 episodes of training the average reward was: 0.651 .\n",
      "After 5000 episodes of training the average reward was: 0.666 .\n",
      "After 6000 episodes of training the average reward was: 0.678 .\n",
      "After 7000 episodes of training the average reward was: 0.68 .\n",
      "After 8000 episodes of training the average reward was: 0.678 .\n",
      "After 9000 episodes of training the average reward was: 0.696 .\n",
      "After 10000 episodes of training the average reward was: 0.645 .\n",
      "After 11000 episodes of training the average reward was: 0.668 .\n",
      "After 12000 episodes of training the average reward was: 0.672 .\n",
      "After 13000 episodes of training the average reward was: 0.692 .\n",
      "After 14000 episodes of training the average reward was: 0.645 .\n",
      "After 15000 episodes of training the average reward was: 0.668 .\n",
      "After 16000 episodes of training the average reward was: 0.673 .\n",
      "After 17000 episodes of training the average reward was: 0.64 .\n",
      "After 18000 episodes of training the average reward was: 0.649 .\n",
      "After 19000 episodes of training the average reward was: 0.701 .\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "for episode in tqdm(range(total_episodes)):\n",
    "    state = environment.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # First the algorithm must decide whether or not to take a random\n",
    "            # action to explore or the best possible action to exploit.\n",
    "            \n",
    "        exploration_exploitation_value = random.uniform(0,1)\n",
    "        \n",
    "        if exploration_exploitation_value > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = environment.action_space.sample()\n",
    "                \n",
    "        # After choosing an action we update the environment with that action.\n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "\n",
    "        # Now we must update the q table with the bellman equation.\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state,:]))\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = new_state\n",
    "        \n",
    "        if done: break\n",
    "        \n",
    "    exploration_rate = min_exploration_probability + (max_exploration_probability - min_exploration_probability) * np.exp(-exploration_probability_decay_rate * episode)\n",
    "        \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "n = 1000\n",
    "rewards_per_n_episodes = np.split(np.array(rewards), total_episodes / n)\n",
    "for index, reward in enumerate(rewards_per_n_episodes):\n",
    "    print(\"After {} episodes of training the average reward was: {} .\".format(n * index, str(sum(reward) / n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watch the algorithm play FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "environment.reset()\n",
    "\n",
    "\n",
    "state = environment.reset()\n",
    "step = 0\n",
    "done = False\n",
    "\n",
    "for step in range(max_steps):\n",
    "    action = np.argmax(q_table[state,:])\n",
    "\n",
    "    new_state, reward, done, info = environment.step(action)\n",
    "\n",
    "    environment.render()\n",
    "    state = new_state\n",
    "    \n",
    "    if done: break\n",
    "    \n",
    "    time.sleep(0.2)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "            \n",
    "environment.close()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
